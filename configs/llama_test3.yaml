model:
  base_llm_model: "meta-llama/Llama-3.2-1B-Instruct"
  safety_model: "s-nlp/roberta_toxicity_classifier"
  lora_r: 8
  lora_alpha: 32
  lora_dropout: 0.1
  bias: null
  target_modules: ["q_proj", "v_proj"]

data:
  rl_dataset: "Seungjun/filtered-short-prompt-collective"
  pt_dataset: "Seungjun/unsafe_pt"
  pt_dataset2: "Anthropic/hh-rlhf"
  batch_size: 8
  max_length: 64
  val_split: 0.005
  rl_target_col: "prompt"
  pt_target_col: "text"
  pt_target_col2: "chosen"

training:
  total_steps: 1000
  log_steps: 15
  lr: 0.00002
  beta: 0.1
  gamma: 0
  gamma2: 1.0
  max_grad_norm: 1.5
  max_length: 64
  no_repeat_ngram_size: 4
  checkpoint_dir: "./checkpoints"